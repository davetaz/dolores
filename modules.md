  

# Communicating outputs vs graphical exploratory data analysis techniques

## Outline:

Communicating outputs

Communicating outputs

There’s something breathtaking about data that is communicated well — it’s a lot like marvelling at an engineering masterpiece. Think about how you felt last time you looked at an engineering or architectural masterpiece, how did it make you feel? Did you feel elated? motivated? educated?

Data communicated well can do exactly the same. Alternatively, when not done well you end up suffering a long presentation of poorly presented, cluttered and misleading charts that don’t seem to be relevant to your problem. ,

## Summary:

You can invest in data technologies, collect all the data you can possibly imagine and build countless complex models, but it’s worthless if the findings are not effectively communicated to decision-makers so that action can be taken from the insights. A well designed output, tailored to the audience and goals/objectives of the project is essential to ensure the right action is taken. Creating an output with a compelling headline, narrative and accompanying visual and/or auditory content can help the right information \pop\ in a very short space of time. Think carefully about the design of your outputs, reduce them down to what is essential before being tempted to add lots of data and interactivity that people may never explore.

### Number of blocks: 24

### Number of questions: 7

### Title of blocks

- Communicating outputs vs graphical exploratory data analysis techniques

- Key elements of a compelling story

- Creating a compelling narrative

- How effective is pop out?

- Adding interaction

  

### Title of components:

- Communicating outputs

- Where are we?

- Inspiring with data

- Hans Rosling - 200 years that changed the world

- A century of flight

- Reshaping New York

- Keeping it simple

- Shaping the narrative

- What is the headline?

- What is the story?

- Which chart is best?

- The basics of visualisation design

- Using colour

- Thinking in dimensions

- Rules of interaction

- Summary

  

# Whose responsibility is it to review and validate results?,

## Outline:

Validation and prioritisation of analysis

Validation and prioritisation of analysis

We have reached a critical point. Data has been collected and analysed; turned into information that may or may not be used to inform the solution to our original problem. Before proceeding we must be certain that the analysis is robust and appropriate to be used as part of the solution.

The validation and prioritisation of analysis stage is key to sign-off our work.,

## Summary:

Data analysis requires a flexible approach. Data analysis typically forms part of “discover and define requirements stage”. For this reason it is more like research and findings may result in significant changes of direction for the project. It is critical to validate, review and approve data analysis before it is taken forward.We operate a three signatures policy for technical diagrams. The same can apply to all stages of data analysis. There are many techniques that can be used to validate analysis. In this module we have touched upon a few relevant to this programme. Others will come from experts who have the insight and experience in specific domains. Remember, it is our skills and capability, coupled with our expertise, data, and the insight it generates that enables us to provide intelligent solutions.

Number of blocks: 16

### Number of questions: 8

### Title of blocks

- Whose responsibility is it to review and validate results?,

- The differences between data analysis and technical drawings,

- Verifying your research,

- Verification methods,

- What happens when it doesn’t work in the real world?,

- Verifying the system that prepares data,

- Prioritising analysis through open research,

  

### Title of components:

- Validation and prioritisation of analysis,

- Verifying the model,

- Verifying the outcome,

- Collecting new data,

- Verifying newly collected data,

- Iterate to improve,

- Completing the verification,

- Summary,

  

# Statistics,

## Outline:

Inferential statistics & machine learning

Predicting the future is hard. The understanding of Inferential statistics and machine learning is critical to make sure that any models we are building and insights we generate are going to be sound when applied.

In this module we fill look at methods that enable making inferences about the future by learning common statistics and machine learning techniques.

This module covers:

- Types of inferential statistics such as hypothesis testing, overfitting, and correlations

- Machine learning techniques such as types of algorithms, and metrics to evaluate predictions,

## Summary:

Predicting the future is hard. The understanding of Inferential statistics and machine learning is critical to make sure that any models we are building and insights we generate are going to be sound when applied to the wider population, especially when looking to avoid future disaster. In this module we fill looked inferential statistics and machine learning. Two highly interconnected topics and look to not only build a model to predict the future but also adapt to changes as new insight is discovered. Having knowledge of these areas is not sufficient however. Expert domain knowledge is required to guide the models and set parameters and rules that ensure they don't go badly wrong!

Number of blocks: 33

### Number of questions: 15

### Title of blocks

- Statistics

- Evaluating our impact on wildlife

- Key inferential statistics techniques

- Confidence intervals and normal distribution

- Regression Analysis

- What is the most significant cause of overfitting?

- Correlation

- Correlation and causation examples

- Machine Learning

- Applications of machine learning

- Precision and Recall

- Machine learning - clustering

- Variable section

- Incorrect variable selection

- Machine learning types

  

### Title of components:

- Inferential statistics & machine learning

- Descriptive and inferential statistics

- Inferential and descriptive statistics

- Categorise the following accordingly

- Inferential statistics and sampling

- Establishing seismic risk of buildings

- Hypothesis testing

- A coin flip: the null hypothesis

- A coin flip: the alternate hypothesis

- You have to be certain

- Salt concentration and yield

- Every answer is possible

- Overfitting

- Avoiding overfitting

- Railway inspection

- Machine learning in action

- Precision and recall

- Summary

  

# Why Exploratory Data Analysis?,

## Outline:

Exploratory Data Analysis

Exploratory Data Analysis

The analysis that we conduct in the exploratory phase is defined by the scope of the project. But exploratory data analysis is part of every stage in the project lifecycle where data is involved as exploring the information it holds help us shape our decisions.

This module will give you the practical skills you need to understand how to best conduct an EDA by focussing on three main aspects:

- Establishing the type and features of data

- Descriptive methods to analyse data

- Graphical methods to analyse data

## Summary:

Exploratory data analysis (EDA) is an approach used to understand and analyse the main characteristics of a dataset. EDA will often use a combination of descriptive and graphical methods in order to: There are a wide variety of techniques that can be used to perform EDA, each suited for different types of data and objectives. Pick carefully when doing your own EDA; even picking the wrong measure of average can lead to problems!

Number of blocks: 33

### Number of questions: 11

### Title of blocks

- Why Exploratory Data Analysis?

- Data types

- Emergency service performance

- The importance of descriptive statistics

- In what way does the visualisation here mislead the viewer?

- In what way does the visualisation here mislead the viewer?

- Area or stacked area chart?

  

### Title of components:

- Exploratory Data Analysis

- 1. Establishing data types and features

- Dimensionality

- Sampling

- Fair sampling?

- Sampling techniques

- Which technique?

- 2. Descriptive methods to analyse data

- Measures of Central Tendency

- averages

- Outliers and robustness

- distribution of the data

- The distribution of data

- Standard deviations

- box-and-whisker plot

- Box-and-whisker plot

- Flight delays

- 3. Graphical methods to analyse data

- Bar chart

- Pie Chart

- Histogram

- Box and whisker plot

- Line chart

- Scatter-plot

- Some graphs are less clear than others...

- Pareto chart

- Area and stacked area charts

- 3D charts and bubble charts

- Stream graph

- Test your knowledge

- Choosing the right graph

- title

- Summary

  

## Where are we?

## Outline:

Cleaning & Transforming Data

Cleaning & Transforming Data

Before data can be used as part of a project, the quality and fitness for purpose of that data needs to be checked. There are three main stages in this process:

Data cleaning - Identifying and correcting issues with data

Data transformation - Converting the data to different formats or structures

Exploratory data analysis - Examining the dataset to identify bias and test hypothesis.

This module examines the process of cleaning and transforming data ready for analysis. Data cleaning is the essential first step. Exploratory data analysis may happen before or after data transformation. ,

## Summary:

Before data can be used as part of a project, the quality and fitness for purpose of that data needs to be checked. It should never be assumed that data is suitable for the intended use. Every dataset should be checked for accuracy, completeness, duplicates and currency. In addition the data will most likely need to be transformed or combined with other data to make the data conform to specific quality standards. The process of cleaning and transforming data can take substantial time. Depending on the scale, rate and method of acquisition of data it might be advisable to automate the cleaning and transforming process. Additionally, if might be necessary to go back a stage in your data lifecycle and change the requirements or collection technique to help reduce the errors and inconsistencies in the the data at time of collection. Regardless, a manual validation and fitness for purpose tests will still need to be carried out before conclusions can be drawn from the data. The next module looks at how we can use Exploratory Data Analysis to perform these validation checks.

Number of blocks: 2

### Number of questions: 6

### Title of blocks

- Where are we?

- How many errors can you spot?

- Methods for cleaning data

  

### Title of components:

- Cleaning & Transforming Data,

- Where are we?

- 1. Data cleaning

- Common errors in data

- How to fix errors in data?

- Cleaning data

- Manual semi-manual and automated cleaning

- Dealing with missing data

- Methods for cleaning data

- Keeping track

- 2. Transforming data

- Why transform data?

- Why transform data?

- Considerations when transforming data

- Match the method to the data type

- Match the method to the data type

- Who's responsibility is it to clean and transform data?

- Summary

  

## Data management principles,

## Outline:

Data storage & management

Data storage & management

Data needs a place to rest, to be accessed and used. Choosing the right place to store and manage data depends on many factors.

Think of data like a personal possession in your house. You choose where to put things so they are in the most appropriate place. You wouldn't put a suitcase in a kitchen cabinet. The same analogy applies to data.

This module looks at the different data storage options, and explores the criteria that help make an appropriate choice.

Complete. Intro logo

Intro logo,

## Summary:

Data needs a place to rest, to be accessed and used. Choosing the right place to store and manage data depends on many factors. We must ensure that any choices made conform to our legal, ethical and governance principles. Choices also need to be appropriate for the project and potentially flexible to changes. Thinking about suitable storage should ideally happen at the data requirements stage. Selecting the right method of storage should then be a straightforward process of matching requirements with the ideal solution. ,

Number of blocks: 1

### Number of questions: 5

### Title of blocks

- Data management principles

- Legal and contractual obligations

- data governance principles

- Appropriate for the project need

- Matching data requirements

- Being flexible

- Local data storage

- Central data storage

- Cloud services

- File based storage (e.g. excel)

- Relational databases

- Non-relational databases

- Data warehouses and data lakes

  

### Title of components:

- Data storage & management

- Intro logo

- Handling personal data

- Handling commercially sensitive data

- Data storage methods

- Data storage techniques

- Picking the right data management solution

- Q: Transport data

- Q: Humidity conditions

- Q: Building intelligence dashboard

- Summary

  

# Data usage rights

## Outline:

Data requirements

Data requirements

From managing the company to running and delivering projects, data can play a key role in informing us on the best decisions to make.

To get the most from data it is necessary to assess if data can help us solve the problem. This starts with evaluating our rights and legal obligations associated with the data in question.

Following this, we need to set requirements that ensure the data is fit for purpose. This requires an understanding of the fundamental terminology and key concepts related to data.

This module will cover the important characteristics of data and how these shape the requirements for using data in projects.,

## Summary:

Data: Quantities, characters or symbols etc on which analysis can be performed. Dataset: A collection of data that is treated as a single unit (e.g collection of records). Degree of structure: How structured or unstructured the data is. Structured data: Any data stored in a consistent, fixed format. Semi-structured data: Organised by does not obey a formal fixed structure. Unstructured data: Not organised in any pre-defined manner. Degree of openness: Is the data open, shared or closed. Terms of use: State what we are allowed to do with the data. Attribute: The name for a specific property of feature of a data record. ,

Number of blocks: 25

### Number of questions: 11

### Title of blocks

- Data usage rights

- Defining your requirements

- Essential terminology

  

### Title of components:

- Data requirements

- Data usage rights

- Test your knowledge

- Assessing data requirements

- Data types

- Decoding a dataset

- Attributes values and metadata

- Basic classes of data

- Schemas

- Setting your requirements

- Test your knowledge

- Characteristics of datasets

-- Structured semi-structured and unstructured data

- Test your knowledge

- Relationships matter

- Test your knowledge

- Size

- Test your knowledge

  

# Working with data

## Outline:

The environment can change people’s lives for the better, and we can shape a better world. Every day, everywhere, we strive to meet this aim helping clients solve their most complex challenges by harnessing our diverse skills and constantly expanding what’s technically possible.

To solve these challenges there may be many suitable approaches; The question is, how do we assess the one that is most appropriate?

This course looks at how data can play a key role in not only assessing the most appropriate solutions, but also be part of the solution. ,

### Summary:

It is important for business to help clients understand and address their problems through effective solutions. The four stage extended double diamond methodology, facility be data, can help us deliver the right solutions to our clients. We have incorporated the four stage methodology in our process because it’s iterative nature helps us maintain our focus on the problem. And allows us to assess if our solutions are progressing in the right direction, with the appropriate tools. We are already data rich and undertake a wide variety of analysis with the data we have access to. There is a huge opportunity to do even more with data, but we must be able to recognise when data won't help, or worse, lead to inappropriate solutions. The rest of this course focuses on how to assess which data is most adequate, the ways in which it can be acquired, how to check if the data has the right quality, how data is analysed, and how are its outputs communicated. Knowing the basics will help you get the most from data in your projects.,

Number of blocks: 16

### Number of questions: 13

### Title of blocks

- Working with data

- How we work

- Our methodology

- Completing the project brief

- Stage 2: Discover and define requirements

- The role of data in stage 2

- Reacting to change

- Stage 3: Design and build solution

- Stage 4: Support and maintain

- The role of data in stage 4

- Transitioning between stages

- Being iterative

- Stakeholder engagement

- Case study a data approach

- Data collection points

- Checking data integrity and quality

- Finding the right solution

- Proactive protection

- The common stages surrounding data use in projects

- Stage 1: Defining the question & scope

  

### Title of components:

- Data is not new

- Course overview

- Our approach: All four stages

- Stage 2 & 3

- Planning your time

- Summary
